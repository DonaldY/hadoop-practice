## 一、需求


假设点击日志文件( `click.log`)中每行记录格式如下:
```
INFO 2019-09-01 00:29:53 requestURI:/click?app=1&p=1&adid=18005472&industry=469&adid=31
INFO 2019-09-01 00:30:31 requestURI:/click?app=2&p=1&adid=18005472&industry=469&adid=31
INFO 2019-09-01 00:31:03 requestURI:/click?app=1&p=1&adid=18005472&industry=469&adid=32
INFO 2019-09-01 00:31:51 requestURI:/click?app=1&p=1&adid=18005472&industry=469&adid=33
```


另有曝光日志(`imp.log`)格式如下: 
```
INFO 2019-09-01 00:29:53 requestURI:/imp?app=1&p=1&adid=18005472&industry=469&adid=31
INFO 2019-09-01 00:29:53 requestURI:/imp?app=1&p=1&adid=18005472&industry=469&adid=31
INFO 2019-09-01 00:29:53 requestURI:/imp?app=1&p=1&adid=18005472&industry=469&adid=34
```


实现：
1. 用 `Spark-Core` 实现统计每个 `adid` 的曝光数与点击数，将结果输出到 `hdfs` 文件; 输出文件结构为 `adid`、曝光数、点击数。
    > 注意: 数据不能有丢失(存在某些 `adid` 有 `imp`，没有 `clk` 或有 `clk` 没有 `imp` )
    
2. 你的代码有多少个 `shuffle`，是否能减少? (提示:仅有1次 `shuffle` 是最优的)




 



## 二、实现 


```scala

import java.util.StringJoiner

import org.apache.spark.{SparkConf, SparkContext}

object ClickImpLog {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("ClickImpLog").setMaster("local")
    val sc = new SparkContext(conf)
    sc.setLogLevel("warn")

    val clickLog = sc.textFile("data/click.log")
    val impLog = sc.textFile("data/imp.log")

    val clkRDD = clickLog.map { line =>
      val arr = line.split("\\s+")
      val adid = arr(3).substring(arr(3).lastIndexOf("=") + 1)
      (adid, 1)
    }.reduceByKey(_ + _)
    
    val impRDD = impLog.map { line =>
      val arr = line.split("\\s+")
      val adid = arr(3).substring(arr(3).lastIndexOf("=") + 1)
      (adid, 1)
    }.reduceByKey(_ + _)

    // 保存文件到hdfs
    clkRDD.fullOuterJoin(impRDD)
      .map(x => x._1 + "," + x._2._1.getOrElse(0) + "," + x._2._2.getOrElse(0))
      .saveAsTextFile("hdfs://linux121:9000/data/")

    sc.stop()
  }
}
```



一共2个 `shuffle`。`join` 可能产生 `shuffle`，也可能不产生 `shuffle`。


```scala
package com.lagou.spark.anwser

import org.apache.spark.{SparkConf, SparkContext}

// 实际是个问答题

// 点击日志文件(click.log)
// INFO 2019-09-01 00:29:53 requestURI:/click?app=1&p=1&adid=18005472&industry=469&adid=31
// INFO 2019-09-01 00:30:31 requestURI:/click?app=2&p=1&adid=18005472&industry=469&adid=31
// INFO 2019-09-01 00:31:03 requestURI:/click?app=1&p=1&adid=18005472&industry=469&adid=32
// INFO 2019-09-01 00:31:51 requestURI:/click?app=1&p=1&adid=18005472&industry=469&adid=33

// 曝光日志文件(imp.log)
// INFO 2019-09-01 00:29:53 requestURI:/imp?app=1&p=1&adid=18005472&industry=469&adid=31
// INFO 2019-09-01 00:29:53 requestURI:/imp?app=1&p=1&adid=18005472&industry=469&adid=31
// INFO 2019-09-01 00:29:53 requestURI:/imp?app=1&p=1&adid=18005472&industry=469&adid=34

// 1、数据抽取
// 点击日志 => (18005472, 1) id为 18005472 的广告被点击了一次
// 曝光日志 => (18005472, 1) id为 18005472 的广告被曝光了一次

// 2、汇总
// 点击日志 => (18005472, 1) id为 18005472 的广告被点击了一次 => reduceByKey => (adid1, count1) adid1被点击了count1次
// 曝光日志 => (18005472, 1) id为 18005472 的广告被曝光了一次 => reduceByKey => (adid1, count2) adid1被曝光了count2次

// 3、连接
// (adid1, count1) join (adid1, count2) => (adid1, count1, count2) => adid1被点击了count1次，曝光了count2次

// 分析：以上过程是一个最普通的过程，全程有 3 次 Shuffle
// 做以下修改可以只有 1 次 Shuffle
// 1、数据抽取
// 点击日志 => (18005472, (1, 0))
// 曝光日志 => (18005472, (0, 1))
// 2、汇总
// (18005472, (1, 0)) ... ... (18005472, (0, 1)) => reduceByKey

class HomeWork3_ad {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("HomeWork3_ad").setMaster("local")
    val sc = new SparkContext(conf)
    sc.setLogLevel("warn")

    val clickLog = sc.textFile("data/click.log")
    val impLog = sc.textFile("data/imp.log")

    val clkRDD = clickLog.map { line =>
      val arr = line.split("\\s+")
      val adid = arr(3).substring(arr(3).lastIndexOf("=") + 1)
      (adid, (1, 0))
    }

    val impRDD = impLog.map { line =>
      val arr = line.split("\\s+")
      val adid = arr(3).substring(arr(3).lastIndexOf("=") + 1)
      (adid, (0, 1))
    }

    clkRDD.union(impRDD)
      .reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))
      .foreach(println)

    sc.stop()
  }
}
```
