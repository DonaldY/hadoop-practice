## 一、日志分析


日志格式: `IP` 命中率(`Hit/Miss`) 响应时间 请求时间 请求方法 请求URL 请求协议 状态码 响应大小 `referer` 用户代理

日志文件位置: `data/cdn.txt`

```
100.79.121.48 HIT 33 [15/Feb/2017:00:00:46 +0800] "GET http://cdn.v.abc.com.cn/videojs/video.js HTTP/1.1" 200 174055 "http://www.abc.com.cn/" "Mozilla/4.0+(compatible;+MSIE+6.0;+Windows+NT+5.1;+Trident/4.0;)"
```


术语解释:
• `PV(page view)`，即页面浏览量;衡量网站或单一网页的指标
• `uv(unique visitor)`，指访问某个站点或点击某条新闻的不同 `IP` 地址的人数


任务:
1. 计算独立 `IP` 数
2. 统计每个视频独立 `IP` 数(视频的标志:在日志文件的某些可以找到 `*.mp4`，代表一个视频文件)
3. 统计一天中每个小时的流量


```scala
package com.lagou.spark.anwser

import java.util.regex.Pattern
import org.apache.spark.{SparkConf, SparkContext}

object HomeWork2_log {
  val ipPattern = Pattern.compile("""(\S+) .+/(\S+\.mp4) .*""")
  val flowPattern = Pattern.compile(""".+ \[(.+?) .+ (200|206|304) (\d+) .+""")

  def main(args: Array[String]): Unit = {
    // 初始化
    val conf = new SparkConf().setAppName("BaseStationDemo").setMaster("local[*]")
    val sc = new SparkContext(conf)
    sc.setLogLevel("WARN")

    // 读并解析 log 信息
    val logRDD = sc.textFile("data/cdn.txt")

    // 1.计算独立IP数前10名 及 独立IP数
    val ipRDD = logRDD.map(line => (line.split("\\s+")(0), 1))
      .reduceByKey(_ + _)
      .sortBy(_._2, false)

    ipRDD.take(10).foreach(println)
    println(s"独立IP数：${ipRDD.count()}")

    // 2.统计每个视频独立IP数
    println("视频独立IP数:")
    logRDD.map(line => {
      val matchFlag = ipPattern.matcher(line)
      if (matchFlag.matches())
        ((matchFlag.group(2), matchFlag.group(1)), 1)
      else
        (("", ""), 0)
    }).filter{case ((video, ip), count) => video != "" && ip != "" && count != 0}
      .reduceByKey(_ + _)
      .map { case ((video, _), _) => (video, 1) }
      .reduceByKey(_ + _)
      .sortBy(_._2, false)
      .take(10)
      .foreach(println)

    // 3.统计一天中每小时的流量
    println("每小时流量:")
    logRDD.map(line => {
      val matchFlag = flowPattern.matcher(line)
      if (matchFlag.matches())
        (matchFlag.group(1).split(":")(1), matchFlag.group(3).toLong)
      else
        ("", 0L)
    }).filter{case (hour, flow) => flow != 0}
      // 数据量很小，可以收到一个分区中做reduce，然后转为集合操作效率高
      .reduceByKey(_ + _, 1)
      .collectAsMap()
      .mapValues(_ / 1024 / 1024 / 1024)
      .toList
      .sortBy(_._1)
      .foreach { case (k, v) => println(s"${k}时 CDN流量${v}G") }

    sc.stop()
  }
}
```
