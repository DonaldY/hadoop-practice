## 一、找到 `ip` 所属区域

`http.log` : 用户访问网站所产生的日志。

日志格式为:时间戳、IP地址、访问网址、访问数据、浏览器信息等

`ip.dat` : `ip` 段数据，记录着一些 `ip` 段范围对应的位置

文件位置: `data/http.log`、`data/ip.dat`

```
# http.log样例数据。格式:时间戳、IP地址、访问网址、访问数据、浏览器信息
20090121000132095572000|125.213.100.123|show.51.com|/shoplist.php?phpfile=shoplist2.php&style=1&sex=137|Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; Mozilla/4.0(Compatible Mozilla/4.0(Compatible-EmbeddedWB 14.59 http://bsalsa.com/ EmbeddedWB- 14.59  from: http://bsalsa.com/ )|http://show.51.com/main.php|

# ip.dat数据样例
122.228.96.0|122.228.96.255|2061787136|2061787391|亚洲|中国|浙江|温州||电 信|330300|China|CN|120.672111|28.000575
```


要求: 将 `http.log` 文件中的 `ip` 转换为地址。如将 `122.228.96.111` 转为温州，并统计各城市的总访问量。


思路分析：

1. 定义方法：根据 `ip` 判断该 `ip` 是否属于某个网段，将 `ip` 转为 `long` 类型数值，比较大小即可。
2. 拿到 `http.log` 文件中所有数据的 `ip` 地址，然后根据 `ip.dat` 文件数据判断是否属于网段内，从而得到 `ip` 和城市的映射数据 `ip_Addr`
3. 为了减少计算的数据量和计算正确性，需先保证 `http.log` 中的 `ip` 去重
4. 将 `http.log` 中的数据左连接 `ip_Addr`，将数据中的 `ip` 替换为城市
5. 根据城市分组统计每组数据量则得到各个城市访问量



优化处理：

1. `groupByKey` 性能不好，所以尽量避免使用 `groupByKey`
2. `shuffle` 会导致 `spark` 程序性能差，所以尽量避免 `shuffle`
3. 对重复使用的 `RDD` 进行缓存


```scala

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object Ip2Address2 {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Ip2Address2").setMaster("local[*]")
    val sc = new SparkContext(conf)
    sc.setLogLevel("warn")

    val httpLines: RDD[String] = sc.textFile("data/http.log")
    val ipLines: RDD[String] = sc.textFile("data/ip.dat")

    // 解析ip.dat数据：（开始ip,结束ip,网段所属城市）
    val ipAddrRDD = ipLines.map { line =>
      val fields = line.split("\\|")
      (fields(0), fields(1), fields(7))
    }

    // 解析http.log数据：(ip, 一行数据)
    val httpRDD = httpLines.map { line =>
      val fields = line.split("\\|")
      (fields(1), line)
    }

    // ip去重
    val logIpRdd = httpRDD.map(_._1).distinct()

    // 求出日志中的ip和地址的映射数据集(ip,城市)
    val ip_Addr = ipAddrRDD.cartesian(logIpRdd)
      .filter {
        // 逐条判断ip地址是否在网段中，只保留ip地址在所属网段的数据
        case ((startIp, endIp, _), ip) => ipInRange(ip, startIp, endIp)
      }.map{
        case ((_, _, addr), ip) => (ip, addr)
      }

    //（城市,IP转为城市后的数据）使用广播变量，避免了shuffle
    val bc = sc.broadcast(ip_Addr.collectAsMap())
    val cityRdd = httpRDD.map{case (ip, line) =>
      val city = bc.value.getOrElse(ip, null)
      (city, line.replace(ip, city))
    }

    // 缓存
    cityRdd.cache()

    // 统计每个key数据条数即为各城市的总访问量（用countByKey替代了groupBykey）
    // cityRdd.mapValues(_ => 1).reduceByKey(_+_).foreach(x => println(x._1 + ":" + x._2))
    cityRdd.countByKey().foreach(x => println(x._1 + ":" + x._2))

    // IP转为地址后的数据输出到文件
    cityRdd.map(_._2).saveAsTextFile("data/logout2")

    sc.stop()
  }

  /**
   * 判断ip是否在网段内
   * @param IP
   * @param startIP 开始IP
   * @param endIP 结束IP
   * @return
   */
  def ipInRange(IP: String, startIP: String, endIP: String): Boolean = {
    getIp2long(startIP) <= getIp2long(IP) && getIp2long(IP) <= getIp2long(endIP)
  }
  
  def getIp2long(ip: String): Long = {
    val ips = ip.split("\\.")
    var ip2long = 0L
    for (i <- 0 until 4) {
      ip2long = ip2long << 8 | Integer.parseInt(ips(i))
    }
    ip2long
  }
}
```



核心思想：

1. 对结果进行排序，使用二分法查找提高效率
2. 对比较固定的数据设置为广播变量，提高效率
3. 数据量大时考虑使用 `mapPartition` 替代 `map`

实现方法1（`Spark Core`）：
```scala

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object HomeWork1RDD_ip {
  def main(args: Array[String]): Unit = {
    // 初始化
    val conf = new SparkConf().setAppName(this.getClass.getCanonicalName).setMaster("local[*]")
    val sc = new SparkContext(conf)
    sc.setLogLevel("WARN")

    // 读数据，并解析。将ip地址转换为Long类型
    val httpData: RDD[Long] = sc.textFile("data/http.log")
      .map(x => ip2Long(x.split("\\|")(1)))

    // 读数据，解析，收回来。最后变为广播变量
    val ipData: Array[(Long, Long, String)] = sc.textFile("data/ip.dat")
      .map { line =>
        val field = line.split("\\|")
        (field(2).toLong, field(3).toLong, field(6))
      }.collect()
    // 此处对ipData进行排序后再广播，方便后面使用二分法查找城市
    val ipBC = sc.broadcast(ipData.sortBy(_._1))

    // 逐条数据比对，找到对应的城市。使用二分查找
    httpData.mapPartitions { iter =>
      val ipsInfo: Array[(Long, Long, String)] = ipBC.value
      iter.map { ip =>
        val city: String = getCityName(ip, ipsInfo)
        (city, 1)
      }
    }.reduceByKey(_ + _)
      .collect()
      .foreach(println)

    sc.stop()
  }

  // 将ip地址转换为Long。有多种实现方法，此方法使用了位运算，效率最高
  def ip2Long(ip: String): Long = {
    ip.split("\\.")
      .map(_.toLong)
      .fold(0L) { (buffer, elem) =>
        buffer << 8 | elem
      }
  }

  // 给定ip地址，在ips中查找对应的城市名。使用二分查找算法
  def getCityName(ip: Long, ips: Array[(Long, Long, String)]): String = {
    var start = 0
    var end = ips.length - 1
    var middle = 0

    while (start <= end) {
      middle = (start + end) / 2
      if ((ip >= ips(middle)._1) && (ip <= ips(middle)._2))
        return ips(middle)._3
      else if (ip < ips(middle)._1)
        end = middle - 1
      else
        start = middle + 1
    }
    "Unknown"
  }
}
```


实现方法2（`Spark SQL`）：
```scala
package com.lagou.spark.anwser

import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf

object HomeWork1DSL_ip {
  def main(args: Array[String]): Unit = {
    // 初始化
    val conf = new SparkConf().setAppName(this.getClass.getCanonicalName).setMaster("local[*]")
    val spark: SparkSession = SparkSession.builder()
      .config(conf)
      .getOrCreate()
    spark.sparkContext.setLogLevel("WARN")

    // 读数据，并解析。将ip地址转换为Long类型
    import spark.implicits._
    spark.read
      .option("delimiter", "|")
      .csv("data/http.log")
      .map(row => row.getString(1))
      .createOrReplaceTempView("t1")

    // 读数据，解析，收回来。最后变为广播变量
    val ipData: Array[(Long, Long, String)] = spark.read
      .option("delimiter", "|")
      .csv("data/ip.dat")
      .map(row => (row.getString(2).toLong, row.getString(3).toLong, row.getString(6)))
      .collect()
    val ipBC = spark.sparkContext.broadcast(ipData.sortBy(_._1))

    def ip2Long(ip: String): Long = {
      ip.split("\\.")
        .map(_.toLong)
        .fold(0L) { (buffer, elem) =>
          buffer << 8 | elem
        }
    }

    // 给定ip地址，在ips中查找对应的城市名。使用二分查找算法
    def getCityName(ip: Long): String = {
      val ips: Array[(Long, Long, String)] = ipBC.value
      var start = 0
      var end = ips.length - 1
      var middle = 0

      while (start <= end) {
        middle = (start + end) / 2
        if ((ip >= ips(middle)._1) && (ip <= ips(middle)._2))
          return ips(middle)._3
        else if (ip < ips(middle)._1)
          end = middle - 1
        else
          start = middle + 1
      }
      "Unknown"
    }

    spark.udf.register("ip2Long", ip2Long _)
    spark.udf.register("getCityName", getCityName _)
    spark.sql(
      """
        |select getCityName(ip2Long(value)) as provice, count(1) as no
        |  from t1
        |group by getCityName(ip2Long(value))
        |""".stripMargin).show

    spark.close()
  }
}
```
